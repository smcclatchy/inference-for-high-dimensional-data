---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 07-false-discovery-rate.md in _episodes_rmd/
source: Rmd
title: "False Discovery Rate"
teaching: 0
exercises: 0
questions:
- "How can you control false discovery rates?"
objectives:
- "Calculate the false discovery rate."
- "Explain the limitations of restricting family wise error rates in a study."
keypoints:
- "..."
- "..."
---

## False Discovery Rate 

There are many situations for which requiring an FWER of 0.05 does not make 
sense as it is much too strict. For example, consider the very common exercise 
of running a preliminary small study to determine a handful of candidate genes. 
This is referred to as a _discovery_ driven project or experiment. We may be in 
search of an unknown causative gene and more than willing to perform follow-up 
studies with many more samples on just the candidates. If we develop a procedure 
that produces, for example, a list of 10 genes of which 1 or 2 pan out as 
important, the experiment is a resounding success. With a small sample size, the 
only way to achieve a FWER $\leq$ 0.05 is with an empty list of genes. We 
already saw in the previous section that despite 1,000 diets being effective, we 
ended up with a list with just 2. Change the sample size to 6 and you very 
likely get 0:


```r
population <- read.csv(file = "../data/femaleControlsPopulation.csv")
m <- 10000
delta <- 3
set.seed(1)
pvals <- sapply(1:m, function(i){
  control <- sample(population, 6)
  treatment <- sample(population, 6)
  if(!nullHypothesis[i]) treatment <- treatment + delta
  t.test(treatment, control)$p.value
  })
```

```
## Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when 'replace = FALSE'
```

```r
sum(pvals < 0.05/10000)
```

```
## Error in eval(expr, envir, enclos): object 'pvals' not found
```

By requiring a FWER $\leq$ 0.05, we are practically assuring 0 power 
(sensitivity). In many applications, this specificity requirement is over-kill. 
A widely used alternative to the FWER is the false discovery rate (FDR). The 
idea behind FDR is to focus on the random variable $Q \equiv V/R$ with $Q=0$ 
when $R=0$ and $V=0$. Note that $R=0$ (nothing called significant) implies $V=0$ 
(no false positives). So $Q$ is a random variable that can take values between 0 
and 1 and we can define a rate by considering the average of $Q$. To better 
understand this concept here, we compute $Q$ for the procedure: call everything 
p-value < 0.05 significant.

> ## Discussion
> Refer to the confusion matrix below showing false positive, false negative,
> and false discovery rates, along with specificity and sensitivity. Turn to a 
> partner and explain the following:  
> How is the false discovery rate related to Type I error and false positive 
> rates?  
> How is sensitivity related to the false discovery rate?  
> When you are finished discussing,  share with the group in the collaborative
> document.
>
> > ## Solution
> >
> > 
> {: .solution}
{: .challenge}

![confusion matrix showing error rates](../fig/confusion-matrix-3.png)

For more on this, see [<i>Classification evaluation</i>](https://www.nature.com/articles/nmeth.3945) by J. Lever, 
M. Krzywinski and N. Altman in <i>Nature Methods</i> <b>13</b>, 603-604 (2016).

#### Vectorizing code

Before running the simulation, we are going to _vectorize_ the code. This means 
that instead of using `sapply` to run `m` tests, we will create a matrix with 
all data in one call to sample. This code runs several times faster than the 
code above, which is necessary here due to the fact that we will be generating 
several simulations. Understanding this chunk of code and how it is equivalent 
to the code above using `sapply` will take a you long way in helping you code 
efficiently in R.


```r
library(genefilter) ##rowttests is here
alpha <- 0.05
N <- 12
set.seed(1)
##Define groups to be used with rowttests
g <- factor( c(rep(0, N), rep(1, N)) )
B <- 1000 ##number of simulations
Qs <- replicate(B, {
  ##matrix with control data (rows are tests, columns are mice)
  controls <- matrix(sample(population, N*m, replace=TRUE), nrow=m)
  
  ##matrix with control data (rows are tests, columns are mice)
  treatments <-  matrix(sample(population, N*m, replace=TRUE), nrow=m)
  
  ##add effect to 10% of them
  treatments[which(!nullHypothesis),] <- treatments[which(!nullHypothesis),] + delta
  
  ##combine to form one matrix
  dat <- cbind(controls, treatments)
  
 calls <- rowttests(dat, g)$p.value < alpha
 R=sum(calls)
 Q=ifelse(R > 0, sum(nullHypothesis & calls)/R, 0)
 return(Q)
})
```

```
## Error in which(!nullHypothesis): object 'nullHypothesis' not found
```

#### Controlling FDR

The code above is a Monte Carlo simulation that generates 10,000 experiments 
1,000 times, each time saving the observed $Q$. Here is a histogram of these 
values:



```r
library(rafalib)
mypar(1, 1)
hist(Qs) ##Q is a random variable, this is its distribution
```

```
## Error in hist(Qs): object 'Qs' not found
```

The FDR is the average value of $Q$

```r
FDR=mean(Qs)
```

```
## Error in mean(Qs): object 'Qs' not found
```

```r
print(FDR)
```

```
## Error in print(FDR): object 'FDR' not found
```

The FDR is relatively high here. This is because for 90% of the tests, the null 
hypotheses is true. This implies that with a 0.05 p-value cut-off, out of 100 
tests we incorrectly call between 4 and 5 significant on average. This combined 
with the fact that we don't "catch" all the cases where the alternative is true, 
gives us a relatively high FDR. So how can we control this? What if we want 
lower FDR, say 5%?

To visually see why the FDR is high, we can make a histogram of the p-values. We 
use a higher value of `m` to have more data from the histogram. We draw a 
horizontal line representing the uniform distribution one gets for the `m0` 
cases for which the null is true. 



```r
set.seed(1)
controls <- matrix(sample(population, N*m, replace=TRUE), nrow=m)
treatments <-  matrix(sample(population, N*m, replace=TRUE), nrow=m)
treatments[which(!nullHypothesis),] <- treatments[which(!nullHypothesis),] + delta
```

```
## Error in which(!nullHypothesis): object 'nullHypothesis' not found
```

```r
dat <- cbind(controls, treatments)
pvals <- rowttests(dat, g)$p.value 
```

```
## Error in rowcoltt(x, fac, tstatOnly, 1L, na.rm): Invalid argument 'x': must be a real matrix.
```

```r
h <- hist(pvals, breaks=seq(0,1,0.05))
```

```
## Error in hist(pvals, breaks = seq(0, 1, 0.05)): object 'pvals' not found
```

```r
polygon(c(0,0.05,0.05,0), c(0,0,h$counts[1],h$counts[1]), col="grey")
```

```
## Error in xy.coords(x, y, setLab = FALSE): object 'h' not found
```

```r
abline(h=m0/20)
```

```
## Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): object 'm0' not found
```

The first bar (grey) on the left represents cases with p-values smaller than 
0.05. From the horizontal line we can infer that about 1/2 are false positives. 
This is in agreement with an FDR of 0.50.  If we look at the bar for 0.01, we 
can see a lower FDR, as expected, but would call fewer features significant.


```r
h <- hist(pvals,breaks=seq(0,1,0.01))
```

```
## Error in hist(pvals, breaks = seq(0, 1, 0.01)): object 'pvals' not found
```

```r
polygon(c(0,0.01,0.01,0),c(0,0,h$counts[1],h$counts[1]),col="grey")
```

```
## Error in xy.coords(x, y, setLab = FALSE): object 'h' not found
```

```r
abline(h=m0/100)
```

```
## Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): object 'm0' not found
```

As we consider a lower and lower p-value cut-off, the number of features 
detected decreases (loss of sensitivity), but our FDR also decreases (gain of 
specificity). So how do we decide on this cut-off? One approach is to set a 
desired FDR level $\alpha$, and then develop procedures that control the error 
rate: FDR  $\leq \alpha$.

#### Benjamini-Hochberg (Advanced)

We want to construct a procedure that guarantees the FDR to be below a certain 
level $\alpha$. For any given $\alpha$, the Benjamini-Hochberg (1995) procedure 
is very practical because it simply requires that we are able to compute 
p-values for each of the individual tests and this permits a procedure to be 
defined.

For this procedure, order the p-values in increasing order: $p_{(1)},\dots,p_{(m)}$. 
Then define $k$ to be the largest $i$ for which

$$p_{(i)} \leq \frac{i}{m}\alpha$$

The procedure is to reject tests with p-values smaller or equal to  $p_{(k)}$. 
Here is an example of how we would select the $k$ with code using the p-values 
computed above:


```r
alpha <- 0.05
i = seq(along=pvals)
```

```
## Error in seq(along = pvals): object 'pvals' not found
```

```r
mypar(1,2)
plot(i,sort(pvals))
```

```
## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'plot': object 'i' not found
```

```r
abline(0,i/m*alpha)
```

```
## Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): object 'i' not found
```

```r
##close-up
plot(i[1:15],sort(pvals)[1:15],main="Close-up")
```

```
## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'plot': object 'i' not found
```

```r
abline(0,i/m*alpha)
```

```
## Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): object 'i' not found
```

```r
k <- max( which( sort(pvals) < i/m*alpha) )
```

```
## Error in sort(pvals): object 'pvals' not found
```

```r
cutoff <- sort(pvals)[k]
```

```
## Error in sort(pvals): object 'pvals' not found
```

```r
cat("k =",k,"p-value cutoff=",cutoff)
```

```
## Error in cat("k =", k, "p-value cutoff=", cutoff): object 'k' not found
```

We can show mathematically that this procedure has FDR lower than 5%. Please see Benjamini-Hochberg (1995) for details. An important outcome is that we now have 
selected 11 tests instead of just 2. If we are willing to set an FDR of 50% 
(this means we expect at least 1/2 our genes to be hits), then this list grows 
to 1063. The FWER does not provide this flexibility since any list of 
substantial size will result in an FWER of 1.

Keep in mind that we don't have to run the complicated code above as we have 
functions to do this. For example, using the p-values `pvals` computed above,
we simply type the following:

```r
fdr <- p.adjust(pvals, method="fdr")
```

```
## Error in p.adjust(pvals, method = "fdr"): object 'pvals' not found
```

```r
mypar(1,1)
plot(pvals,fdr,log="xy")
```

```
## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'plot': object 'pvals' not found
```

```r
abline(h=alpha,v=cutoff) ##cutoff was computed above
```

```
## Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): object 'cutoff' not found
```

We can run a Monte-Carlo simulation to confirm that the FDR is in fact lower 
than .05. We compute all p-values first, and then use these to decide which get 
called.


```r
alpha <- 0.05
B <- 1000 ##number of simulations. We should increase for more precision
res <- replicate(B,{
  controls <- matrix(sample(population, N*m, replace=TRUE),nrow=m)
  treatments <-  matrix(sample(population, N*m, replace=TRUE),nrow=m)
  treatments[which(!nullHypothesis),]<-treatments[which(!nullHypothesis),]+delta
  dat <- cbind(controls,treatments)
  pvals <- rowttests(dat,g)$p.value 
  ##then the FDR
  calls <- p.adjust(pvals,method="fdr") < alpha
  R=sum(calls)
  Q=ifelse(R>0,sum(nullHypothesis & calls)/R,0)
  return(c(R,Q))
})
```

```
## Error in which(!nullHypothesis): object 'nullHypothesis' not found
```

```r
Qs <- res[2,]
```

```
## Error in eval(expr, envir, enclos): object 'res' not found
```

```r
mypar(1,1)
hist(Qs) ##Q is a random variable, this is its distribution
```

```
## Error in hist(Qs): object 'Qs' not found
```

```r
FDR=mean(Qs)
```

```
## Error in mean(Qs): object 'Qs' not found
```

```r
print(FDR)
```

```
## Error in print(FDR): object 'FDR' not found
```

The FDR is lower than 0.05. This is to be expected because we need to be 
conservative to ensure the FDR $\leq$ 0.05 for any value of $m_0$, such as for 
the extreme case where every hypothesis tested is null: $m=m_0$. If you re-do 
the simulation above for this case, you will find that the FDR increases. 

We should also note that in ...

```r
Rs <- res[1,]
```

```
## Error in eval(expr, envir, enclos): object 'res' not found
```

```r
mean(Rs==0) * 100
```

```
## Error in mean(Rs == 0): object 'Rs' not found
```
... percent of the simulations, we did not call any genes significant.

Finally, note that the `p.adjust` function has several options for error rate 
controlling procedures:

```r
p.adjust.methods
```

```
## [1] "holm"       "hochberg"   "hommel"     "bonferroni" "BH"        
## [6] "BY"         "fdr"        "none"
```
It is important to remember that these options offer not just different 
approaches to estimating error rates, but also that different error rates are 
estimated: namely FWER and FDR. This is an important distinction. More 
information is available from:


```r
?p.adjust
```

In summary, requiring that FDR $\leq$ 0.05 is a much more lenient requirement 
FWER $\leq$ 0.05. Although we will end up with more false positives, FDR gives 
us much more power. This makes it particularly appropriate for discovery phase 
experiments where we may accept FDR levels much higher than 0.05.

## Exercises
The following exercises should help you understand the concept of an error 
controlling procedure. You can think of it as defining a set of instructions, 
such as “reject all the null hypothesis for which p-values < 0.0001” or “reject 
the null hypothesis for the 10 features with smallest p-values”. Then, knowing 
the p-values are random variables, we use statistical theory to compute how many 
mistakes, on average, we will make if we follow this procedure. More precisely, 
we commonly find bounds on these rates, meaning that we show that they are 
smaller than some predetermined value.
As described in the text, we can compute different error rates. The FWER tells 
us the probability of having at least one false positive. The FDR is the 
expected rate of rejected null hypothesis.

Note 1: the FWER and FDR are not procedures, but error rates. We will review 
procedures here and use Monte Carlo simulations to estimate their error rates.

Note 2: We sometimes use the colloquial term “pick genes that” meaning “reject 
the null hypothesis for genes that”.

> ## Exercise 1
> We have learned about the family wide error rate FWER. This is the probability 
> of incorrectly rejecting the null at least once. Using the notation in the 
> video, this probability is written like this: Pr(V > 0).
> What we want to do in practice is choose a procedure that guarantees this 
> probability is smaller than a predetermined value such as 0.05. Here we keep 
> it general and, instead of 0.05, we use α.
> We have already learned that the procedure “pick all the genes with p-value <
> 0.05” fails miserably as we have seen that Pr(V > 0) ≈ 1. So what else can we 
> do?
> The Bonferroni procedure assumes we have computed p-values for each test and 
> asks what constant k should we pick so that the procedure “pick all genes with 
> p-value less than k “ has Pr(V > 0) = 0.05. Furthermore, we typically want to 
> be conservative rather than lenient, so we accept a procedure that has 
> Pr(V > 0) ≤ 0.05.
> So the first result we rely on is that this probability is largest when all 
> the null hypotheses are true:
> Pr(V > 0) ≤ Pr(V > 0|all nulls are true)
> or:
> Pr(V > 0) ≤ Pr(V > 0 | m1 = 0) 
> We showed that if the tests are independent then:
> Pr(V > 0|m1) = 1−(1−k)m 
> And we pick k so that 1 − (1 − k)m = α =⇒ k=1−(1−α)1/m
> Now this requires the tests to be independent. The Bonferroni procedure does 
> not make this assumption and, as we previously saw, sets k = α/m and shows 
> that with this choice of k this procedure results in P r(V > 0) ≤ α.
> In R define
> alphas <- seq(0,0.25,0.01)
> Make a plot of α/m and 1 − (1 − α)1/m for various values of m > 1.
> 
> > ## Solution
> > 
> {: .solution}
{: .challenge}

> ## Exercise 2
> To simulate the p-value results of, say 8,792 t-tests for which the null is 
> true, we don’t actually have to generate the original data. We can generate 
> p-values for a uniform distribution like this: `pvals <- runif(8793,0,1)`. 
> Using what we have learned, set the cutoff using the Bonferroni correction and 
> report back the FWER. Set the seed at 1 and run 10,000 simulation.
> 
> > ## Solution
> > 
> {: .solution}
{: .challenge}

