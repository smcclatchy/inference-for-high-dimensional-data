---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 10-principal-component-analysis.md in _episodes_rmd/
source: Rmd
title: "Principal Components Analysis"
teaching: 0
exercises: 0
questions:
- "How can researchers simplify or streamline EDA in high-throughput data sets?"
- "What is principal component analysis (PCA) and when can it be used?"
objectives:
- "Explain the purpose of dimension reduction."
- "Perform a principal components analysis."
keypoints:
- "Edit the ..."
- "Run ..."
math: yes
---

## Dimension Reduction Motivation

Visualizing data is one of the most, if not the most, important step in the 
analysis of high-throughput data. The right visualization method may reveal 
problems with the experimental data that can render the results from a standard 
analysis, although typically appropriate, completely useless. 

Recall that in recent history biologists went from using their eyes or simple 
summaries to categorize results, to having thousands (and now millions) of 
measurements per sample to analyze. Here we will focus on statistical inference 
in the context of high-throughput measurements, also called high-dimensional 
data. 

We have shown methods for visualizing global properties of the columns or rows, 
but plots that reveal relationships between columns or between rows are more 
complicated due to the high dimensionality of data. For example, to compare each 
of the 189 samples to each other, we would have to create, for example, 17,766 
MA-plots. Creating one single scatterplot of the data is impossible since points 
are very high dimensional. 

We will describe powerful techniques for exploratory data analysis based on 
_dimension reduction_. The general idea is to reduce the dataset to have fewer 
dimensions, yet approximately preserve important properties, such as the 
distance between samples. If we are able to reduce down to, say, two dimensions, 
we can then easily make plots.

#### Example: Reducing two dimensions to one

We consider an example with twin heights. Here we simulate 100 two dimensional 
points that represent the number of standard deviations each individual is from 
the mean height. Each point is a pair of twins:

![Simulated twin pair heights.](figure/simulate_twin_heights-1.png)

To help with the illustration, think of this as high-throughput gene expression 
data with the twin pairs representing the $N$ samples and the two heights 
representing gene expression from two genes. 

We are interested in the distance between any two samples. We can compute this 
using `dist`. For example, here is the distance between the two orange points in 
the figure above:


```r
d=dist(t(y))
as.matrix(d)[1,2]
```

```
## [1] 1.140897
```

What if making two dimensional plots was too complex and we were only able to 
make 1 dimensional plots. Can we, for example, reduce the data to a one 
dimensional matrix that preserves distances between points?

If we look back at the plot, and visualize a line between any pair of points, 
the length of this line is the distance between the two points. These lines tend 
to go along the direction of the diagonal. We have seen before that we can 
"rotate" the plot so that the diagonal is in the x-axis by making a MA-plot 
instead:



```r
z1 = (y[1,]+y[2,])/2 #the sum 
z2 = (y[1,]-y[2,])   #the difference

z = rbind( z1, z2) #matrix now same dimensions as y

thelim <- c(-3,3)
mypar(1,2)

plot(y[1,],y[2,],xlab="Twin 1 (standardized height)",
     ylab="Twin 2 (standardized height)",
     xlim=thelim,ylim=thelim)
points(y[1,1:2],y[2,1:2],col=2,pch=16)

plot(z[1,],z[2,],xlim=thelim,ylim=thelim,xlab="Average height",ylab="Difference in height")
points(z[1,1:2],z[2,1:2],col=2,pch=16)
```

![Twin height scatterplot (left) and MA-plot (right).](figure/rotation-1.png)


## Principal Component Analysis 

We have already mentioned principal component analysis (PCA) above. Here we 
provide further mathematical details. 

The animation below illustrates how principal components are calculated from
data. You can imagine that the black line is a rod and each red dashed line is
a spring. The energy of each spring is proportional to its squared length. The
direction of the first principal component is the one that minimises the total
energy of all of the springs. In the animation below, the springs pull the rod,
finding the direction of the first principal component when they reach
equilibrium. We then use the length of the springs from the rod as the first
principal component.
This is explained in more detail on [this Q&A website](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues).

![](../fig/pendulum.gif)


#### Example: Twin heights

We started the motivation for dimension reduction with a simulated example and 
showed a rotation that is very much related to PCA.


![Twin heights scatter plot.](figure/simulate_twin_heights_again-1.png)

Here we explain specifically what are the principal components (PCs).

Let $\mathbf{Y}$ be $2 \times N$ matrix representing our data. The analogy is 
that we measure expression from 2 genes and each column is a sample. Suppose we 
are given the task of finding a  $2 \times 1$ vector $\mathbf{u}_1$ such that 
$\mathbf{u}_1^\top \mathbf{v}_1 = 1$ and it maximizes $(\mathbf{u}_1^\top\mathbf{Y})^\top (\mathbf{u}_1^\top\mathbf{Y})$. This can be viewed as a projection of each 
sample or column of $\mathbf{Y}$ into the subspace spanned by $\mathbf{u}_1$. 
So we are looking for a transformation in which the coordinates show high 
variability.

Let's try $\mathbf{u}=(1,0)^\top$. This projection simply gives us the height of 
twin 1 shown in orange below. The sum of squares is shown in the title.


```r
mypar(1,1)
plot(t(Y), xlim=thelim, ylim=thelim,
     main=paste("Sum of squares :",round(crossprod(Y[1,]),1)))
abline(h=0)
apply(Y,2,function(y) segments(y[1],0,y[1],y[2],lty=2))
```

```
## NULL
```

```r
points(Y[1,],rep(0,ncol(Y)),col=2,pch=16,cex=0.75)
```

<img src="figure/projection_not_PC1-1.png" title="plot of chunk projection_not_PC1" alt="plot of chunk projection_not_PC1"  />

Can we find a direction with higher variability? How about:

$\mathbf{u} =\begin{pmatrix}1\\-1\end{pmatrix}$ ? 
This does not satisfy $\mathbf{u}^\top\mathbf{u}= 1$ so let's instead try
$\mathbf{u} =\begin{pmatrix}1/\sqrt{2}\\-1/\sqrt{2}\end{pmatrix}$ 


```r
u <- matrix(c(1,-1)/sqrt(2),ncol=1)
w=t(u)%*%Y
mypar(1,1)
plot(t(Y),
     main=paste("Sum of squares:",round(tcrossprod(w),1)),xlim=thelim,ylim=thelim)
abline(h=0,lty=2)
abline(v=0,lty=2)
abline(0,-1,col=2)
Z = u%*%w
for(i in seq(along=w))
  segments(Z[1,i],Z[2,i],Y[1,i],Y[2,i],lty=2)
points(t(Z), col=2, pch=16, cex=0.5)
```

![Data projected onto space spanned by (1 0).](figure/projection_not_PC1_either-1.png)

This relates to the difference between twins, which we know is small. The sum of 
squares confirms this.

Finally, let's try:

$\mathbf{u} =\begin{pmatrix}1/\sqrt{2}\\1/\sqrt{2}\end{pmatrix}$ 


```r
u <- matrix(c(1,1)/sqrt(2),ncol=1)
w=t(u)%*%Y
mypar()
plot(t(Y), main=paste("Sum of squares:",round(tcrossprod(w),1)),
     xlim=thelim, ylim=thelim)
abline(h=0,lty=2)
abline(v=0,lty=2)
abline(0,1,col=2)
points(u%*%w, col=2, pch=16, cex=1)
Z = u%*%w
for(i in seq(along=w))
  segments(Z[1,i], Z[2,i], Y[1,i], Y[2,i], lty=2)
points(t(Z),col=2,pch=16,cex=0.5)
```

![Data projected onto space spanned by first PC.](figure/PC1-1.png)

This is a re-scaled average height, which has higher sum of squares. There is a 
mathematical procedure for determining which $\mathbf{v}$ maximizes the sum of 
squares and the SVD provides it for us.

#### The principal components

The orthogonal vector that maximizes the sum of squares:

$$(\mathbf{u}_1^\top\mathbf{Y})^\top(\mathbf{u}_1^\top\mathbf{Y})$$ 
$\mathbf{u}_1^\top\mathbf{Y}$ is referred to as the first PC. The _weights_ 
$\mathbf{u}$ used to obtain this PC are referred to as the _loadings_. Using 
the language of rotations, it is also referred to as the _direction_ of the 
first PC, which are the new coordinates.

To obtain the second PC, we repeat the exercise above, but for the residuals:

$$\mathbf{r} = \mathbf{Y} - \mathbf{u}_1^\top \mathbf{Yv}_1 $$

The second PC is the vector with the following properties: 

$$ \mathbf{v}_2^\top \mathbf{v}_2=1$$

$$ \mathbf{v}_2^\top \mathbf{v}_1=0$$ 

and maximizes  $(\mathbf{rv}_2)^\top \mathbf{rv}_2$.

When $Y$ is $N \times m$ we repeat to find 3rd, 4th, ..., m-th PCs.

#### `prcomp`

R has a function specifically designed to find the principal components. In this 
case, the data is centered by default. The following function: 


```r
pc <- prcomp( t(Y) )
```

The loadings can be found this way:

```r
pc$rotation
```

```
##            PC1        PC2
## [1,] 0.7072304  0.7069831
## [2,] 0.7069831 -0.7072304
```

The equivalent of the variance explained is included in the: 

```r
pc$sdev
```

```
## [1] 1.2542672 0.2141882
```
component.

We take the transpose of `Y` because `prcomp` assumes the previously discussed 
ordering: units/samples in row and features in columns.




