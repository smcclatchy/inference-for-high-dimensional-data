---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 08-direct-approach-false-discovery-rate.md in _episodes_rmd/
source: Rmd
title: "Direct Approach to FDR and q-values"
teaching: 0
exercises: 0
questions:
- "What are multiple comparisons?"
- "Why are multiple comparisons a problem when drawing inferences from high-throughput data?"
- "How we can we address the multiple comparisons problem?"
objectives:
- "Define multiple comparisons and the resulting problems."
- "Describe two ways to deal with multiple comparisons."
keypoints:
- "..."
- "..."
---

## Direct Approach to FDR and q-values (Advanced)

Here we review the results described by John D. Storey in J. R. Statist. Soc. B (2002). One major distinction between Storey's approach and Benjamini and Hochberg's is that we are no longer going to set a $\alpha$ level a priori. Because in many high-throughput experiments we are interested in obtaining some list for validation, we can instead decide beforehand that we will consider all tests with p-values smaller than 0.01. We then want to attach an estimate of an error rate. Using this approach, we are guaranteed to have $R>0$. Note that in the FDR definition above we assigned $Q=0$ in the case that $R=V=0$. We were therefore computing: 

$$
\mbox{FDR} = E\left( \frac{V}{R} \mid R>0\right) \mbox{Pr}(R>0)
$$

In the approach proposed by Storey, we condition on having a non-empty list, which implies $R>0$, and we instead compute the _positive FDR_ 

$$
\mbox{pFDR} = E\left( \frac{V}{R} \mid R>0\right) 
$$

A second distinction is that while Benjamini and Hochberg's procedure controls under the worst case scenario, in which all null hypotheses are true ( $m=m_0$ ), Storey proposes that we actually try to estimate $m_0$ from the data. Because in high-throughput experiments we have so much data, this is certainly possible. The general idea is to pick a relatively high value p-value cut-off, call it $\lambda$, and assume that tests obtaining p-values > $\lambda$ are mostly from cases in which the null hypothesis holds. We can then estimate $\pi_0 = m_0/m$ as: 

$$
\hat{\pi}_0 = \frac{\#\left\{p_i > \lambda \right\} }{ (1-\lambda) m }
$$

There are more sophisticated procedures than this, but they follow the same general idea. Here is an example setting $\lambda=0.1$. Using the p-values computed above we have:


```r
hist(pvals,breaks=seq(0,1,0.05),freq=FALSE)
```

```
## Error in hist(pvals, breaks = seq(0, 1, 0.05), freq = FALSE): object 'pvals' not found
```

```r
lambda = 0.1
pi0=sum(pvals> lambda) /((1-lambda)*m)
```

```
## Error in eval(expr, envir, enclos): object 'pvals' not found
```

```r
abline(h= pi0)
```

```
## Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): object 'pi0' not found
```

```r
print(pi0) ##this is close to the trye pi0=0.9
```

```
## Error in print(pi0): object 'pi0' not found
```

With this estimate in place we can, for example, alter the Benjamini and Hochberg procedures to select the $k$ to be the largest value so that: 

$$\hat{\pi}_0 p_{(i)} \leq \frac{i}{m}\alpha$$

However, instead of doing this, we compute a _q-value_ for each test. If a feature resulted in a p-value of $p$, the q-value is the estimated pFDR for a list of all the features with a p-value at least as small as $p$.

In R, this can be computed with the `qvalue` function in the `qvalue` package:


```r
library(qvalue)
res <- qvalue(pvals)
```

```
## Error in qvalue(pvals): object 'pvals' not found
```

```r
qvals <- res$qvalues
```

```
## Error in eval(expr, envir, enclos): object 'res' not found
```

```r
plot(pvals, qvals)
```

```
## Error in plot(pvals, qvals): object 'pvals' not found
```

we also obtain the estimate of $\hat{\pi}_0$:


```r
res$pi0
```

```
## Error in eval(expr, envir, enclos): object 'res' not found
```
This function uses a more sophisticated approach at estimating $\pi_0$ than what is described above.

#### Note on estimating $\pi_0$
In our experience the estimation of $\pi_0$ can be unstable and adds a step of uncertainty to the data analysis pipeline. Although more conservative, the Benjamini-Hochberg procedure is computationally more stable. 




